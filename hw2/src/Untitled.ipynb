{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# filename\n",
    "DATA_DIR = '../data'\n",
    "TRAIN_ANS_FILE = os.path.join(DATA_DIR, 'ans_train.csv')\n",
    "QUERY_TRAIN_FILE = os.path.join(DATA_DIR, 'query-train.xml')\n",
    "QUERY_TEST_FILE = os.path.join(DATA_DIR, 'query-test.xml')\n",
    "FILE_LIST = os.path.join(DATA_DIR, 'file-list')\n",
    "INV_FILE = os.path.join(DATA_DIR, 'inverted-file')\n",
    "VOCAB_FILE = os.path.join(DATA_DIR, 'vocab.all')\n",
    "OUTPUT_FILE = 'output.csv'\n",
    "\n",
    "# building maps\n",
    "docname2id = dict()\n",
    "with open(FILE_LIST, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        docname2id[line.strip()] = idx\n",
    "id2docname = {y:x for x, y in docname2id.items()}\n",
    "\n",
    "word2id = dict()\n",
    "with open(VOCAB_FILE, 'r') as f:\n",
    "    f.readline()\n",
    "    for idx, line in enumerate(f, 1):\n",
    "        word2id[line.strip()] = idx\n",
    "\n",
    "word_freq = dict()\n",
    "gram_freq = dict()\n",
    "gram2id = dict()\n",
    "with open(INV_FILE, 'r') as f:\n",
    "    idx = 0\n",
    "    while True:\n",
    "        line = f.readline().strip()\n",
    "        if not line:\n",
    "            break\n",
    "        id_1, id_2, doc_count = [int(i) for i in line.split(' ')]\n",
    "        doc_records = defaultdict(int)\n",
    "        for i in range(doc_count):\n",
    "            doc_id, freq = [int(i) for i in f.readline().strip().split(' ')]\n",
    "            doc_records[doc_id] = freq\n",
    "        if id_2 == -1:\n",
    "            word_freq[(id_1)] = doc_records\n",
    "        gram_freq[(id_1, id_2)] = doc_records\n",
    "        gram2id[(id_1, id_2)] = idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2len = defaultdict(int)\n",
    "for word_id, records in word_freq.items():\n",
    "    for doc_id, freq in records.items():\n",
    "        doc2len[doc_id] += freq\n",
    "avdl = sum([length for _, length in doc2len.items()]) / len(doc2len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "BM25_k = 3\n",
    "norm_b = 0.7\n",
    "\n",
    "word_num = len(word2id) + 1\n",
    "doc_num = len(docname2id)\n",
    "weight = np.zeros((doc_num, word_num))\n",
    "for word_id, records in word_freq.items():\n",
    "    IDF = np.log((doc_num + 1) / len(records))\n",
    "    for doc_id, freq in records.items():\n",
    "        TF = (BM25_k + 1) * freq / (freq + BM25_k)\n",
    "        normalizer = 1 - norm_b + norm_b * doc2len[doc_id] / avdl\n",
    "        weight[doc_id, word_id] = TF * IDF / normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['白', '案', '白', '曉', '燕', '綁', '架', '擄', '人', '勒', '贖', '判', '決', '宣', '判', '審', '判', '改', '判', '徒', '刑', '自', '白', '高', '院', '最', '高', '法', '院', '檢', '察', '官', '合', '議', '庭', '證', '據', '一', '審', '二', '審', '陳', '進', '興', '張', '志', '輝', '張', '素', '真']\n",
      "['漢', '語', '拼', '音', '注', '音', '符', '號', '通', '用', '拼', '音', '拼', '音', '系', '統', '羅', '馬', '拼', '音', '街', '道', '譯', '名', '中', '文', '拼', '音', '系', '統', '中', '文', '英', '譯', '系', '統', '注', '音', '第', '二', '式', '統', '一', '標', '準', '音', '標', '母', '語', '國', '際', '化', '教', '育', '部', '行', '政', '院']\n",
      "['職', '棒', '簽', '賭', '案', '職', '棒', '體', '委', '會', '球', '員', '球', '迷', '球', '團', '球', '場', '虧', '損', '票', '房', '戰', '績', '解', '散', '停', '權', '聯', '盟', '賭', '博', '黑', '金', '約', '談', '涉', '賭', '放', '水']\n",
      "['受', '虐', '兒', '家', '庭', '暴', '力', '婚', '姻', '暴', '力', '父', '母', '施', '虐', '兒', '扶', '基', '金', '會', '家', '扶', '中', '心', '統', '計', '兒', '童', '福', '利', '兒', '童', '保', '護', '親', '職', '教', '育']\n",
      "['選', '舉', '候', '選', '人', '中', '選', '會', '電', '視', '台', '錄', '影', '帶', '新', '聞', '局', '廣', '電', '法', '廣', '電', '處', '選', '罷', '法', '有', '線', '電', '視', '競', '選', '廣', '告', '電', '視', '政', '黨', '播', '放', '媒', '體', '違', '法', '處', '分', '罰', '鍰', '審', '查']\n",
      "['聖', '嬰', '現', '象', '傳', '染', '病', '流', '行', '登', '革', '熱', '腸', '病', '毒', '猩', '紅', '熱', '瘧', '疾', '霍', '亂', '疫', '情', '雨', '量', '洪', '水', '氣', '溫']\n",
      "['白', '色', '恐', '怖', '受', '難', '者', '賠', '償', '補', '償', '補', '償', '金', '額', '補', '償', '條', '例', '申', '請', '行', '政', '院', '國', '防', '部', '立', '法', '院', '戒', '嚴', '時', '期', '懲', '治', '叛', '亂', '不', '當', '叛', '亂', '匪', '諜', '審', '判', '案', '件', '受', '裁', '判', '者', '基', '金', '會']\n",
      "['Ｎ', 'Ｂ', 'Ａ', '球', '員', '工', '會', '球', '隊', '聯', '盟', '勞', '資', '糾', '紛', '勞', '資', '談', '判', '協', '議', '簽', '約', '薪', '資', '停', '工', '史', '騰', '博', '德', '條', '款']\n",
      "['集', '會', '遊', '行', '法', '集', '會', '遊', '行', '集', '遊', '法', '憲', '法', '言', '論', '自', '由', '保', '障', '共', '產', '主', '義', '分', '裂', '國', '土', '大', '法', '官', '會', '議', '立', '法', '修', '正', '條', '文']\n",
      "['科', '省', '柯', '省', '科', '索', '沃', '柯', '索', '伏', '難', '民', '難', '民', '潮', '難', '民', '營', '援', '助', '收', '容', '救', '援', '醫', '療', '人', '道', '避', '難', '馬', '其', '頓', '土', '耳', '其', '外', '交', '部', '國', '際', '聯', '合', '國', '紅', '十', '字', '會', '阿', '爾', '巴', '尼', '亞', '裔', '難', '民']\n",
      "['葡', '式', '蛋', '撻', '葡', '式', '蛋', '塔', '澳', '門', '台', '灣', '流', '行', '熱', '潮', '社', '會', '文', '化', '飲', '食', '文', '化', '消', '費', '者', '市', '場', '銷', '售']\n",
      "['Ｂ', 'Ｏ', 'Ｔ', '促', '參', '法', '政', '府', '投', '資', '興', '建', '融', '資', '公', '共', '建', '設', '促', '進', '民', '間', '參', '與', '適', '用', '範', '圍', '公', '共', '工', '程', '獎', '參', '條', '例', '租', '稅', '優', '惠', '甄', '審', '評', '估', '招', '標', '議', '約']\n",
      "['麥', '可', '喬', '丹', '麥', '可', '喬', '登', '退', '休', '飛', '人', '籃', '球', '代', '言', '人', '耐', '吉', '產', '品', '運', '動', '鞋', '運', '動', '產', '業', '形', '象', '銷', '售', '廠', '商', '市', '場', '經', '濟', '股', '價', '美', '國', '股', '市']\n",
      "['棲', '蘭', '山', '檜', '木', '砍', '伐', '枯', '立', '倒', '木', '生', '立', '木', '森', '林', '生', '態', '政', '府', '農', '委', '會', '退', '輔', '會', '森', '林', '開', '發', '處', '森', '保', '處', '林', '務', '局', '生', '態', '保', '育', '經', '營', '政', '策', '監', '督', '水', '土', '保', '持']\n",
      "['兩', '稅', '合', '一', '促', '進', '產', '業', '升', '級', '促', '產', '條', '例', '產', '升', '條', '例', '投', '資', '抵', '減', '抵', '減', '率', '租', '稅', '優', '惠', '租', '稅', '公', '平', '減', '免', '個', '人', '股', '東', '高', '科', '技', '產', '業', '創', '業', '投', '資', '事', '業', '經', '濟', '部', '財', '政', '部', '經', '建', '會', '行', '政', '院']\n",
      "['騎', '乘', '機', '車', '強', '制', '安', '全', '帽', '騎', '士', '警', '方', '文', '通', '部', '內', '政', '部', '取', '締', '執', '法', '抽', '測', '成', '效', '違', '規', '處', '分', '戴', '帽', '率', '路', '口', '肇', '事', '率', '意', '外', '事', '故', '死', '亡', '率', '統', '計']\n",
      "['二', '千', '年', '總', '統', '大', '選', '總', '統', '副', '總', '統', '民', '意', '調', '查', '民', '調', '問', '卷', '受', '訪', '者', '候', '選', '人', '支', '持', '提', '名', '組', '合', '搭', '配', '政', '黨', '投', '票']\n",
      "['新', '三', '不', '政', '策', '柯', '江', '會', '談', '民', '進', '黨', '建', '國', '黨', '許', '信', '良', '林', '義', '雄', '施', '明', '德', '台', '獨', '統', '獨', '獨', '派', '兩', '岸', '中', '國', '美', '國', '中', '共', '大', '陸', '台', '獨', '黨', '綱', '主', '權', '獨', '立', '轉', '型']\n",
      "['美', '濃', '水', '庫', '美', '濃', '水', '庫', '反', '水', '庫', '抗', '爭', '興', '建', '斷', '層', '污', '染', '地', '質', '工', '業', '環', '境', '安', '全', '水', '資', '源', '生', '態', '水', '質', '替', '代', '方', '案']\n",
      "['庫', '藏', '股', '庫', '藏', '股', '制', '度', '股', '市', '股', '份', '股', '票', '股', '價', '股', '東', '護', '盤', '公', '積', '資', '本', '上', '市', '公', '司', '內', '線', '交', '易', '認', '股', '權', '證', '子', '公', '司', '母', '公', '司', '財', '政', '部', '證', '期', '會', '投', '資', '人', '董', '監', '事']\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "tree = ET.ElementTree(file=QUERY_TEST_FILE)\n",
    "root = tree.getroot()\n",
    "query_num = len(root)\n",
    "\n",
    "queries = np.zeros((query_num, word_num))\n",
    "for query_id, child in enumerate(root):\n",
    "    query = list(''.join(child[4].text.strip('\\n。 ').split('、')))\n",
    "    print(query)\n",
    "    for word, freq in Counter(query).items():\n",
    "        IDF = np.log((doc_num + 1) / len(word_freq[word2id[word]]))\n",
    "        TF = (BM25_k + 1) * freq / (freq + BM25_k)\n",
    "        queries[query_id, word2id[word]] = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = np.matmul(queries, np.transpose(weight)).argsort(axis=1)[:, ::-1][:, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "%d format: a number is required, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2bb3c976b3cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'query_id,retrieved_docs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mid2docname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-2bb3c976b3cf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'query_id,retrieved_docs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mid2docname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: %d format: a number is required, not str"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_FILE, 'w+') as f:\n",
    "    print('query_id,retrieved_docs', file=f)\n",
    "    for idx, result in enumerate(ret, 11):\n",
    "        print(str(idx).zfill(3), ' '.join(['doc_%d' % (id2docname[i].split('/')[-1].lower()) for i in result]), sep=',', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
